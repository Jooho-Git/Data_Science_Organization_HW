{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 16기 3주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/xnote/Desktop/투빅스/3주차/wk3_optimization/assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Label       200 non-null    int64  \n",
      " 1   bias        200 non-null    int64  \n",
      " 2   experience  200 non-null    float64\n",
      " 3   salary      200 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 6.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    200\n",
       "Name: bias, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.bias.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28125748, 0.92016503, 0.08086081])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += X[i]*parameters[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \\frac{1}{1+e^{-X_i\\theta}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters)\n",
    "    p = 1/(1+np.exp(-z))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7983026908686086"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = -\\Sigma(y_i\\log p(X_i)+(1-y_i)\\log(1-p(X_i)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X,parameters)\n",
    "    loss = y*np.log(p) + (1-y)*np.log(1-p)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# 내적할 때, 서로 shape가 잘맞는지 확인\n",
    "print(parameters.shape)\n",
    "print(len(np.dot(X_train,parameters)))\n",
    "print(len(np.dot(X_train,parameters.reshape(-1,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_i(X, y, parameters):\n",
    "    y_hat = np.dot(X, parameters)\n",
    "    loss = ((y-y_hat)**2)/2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n : 현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X,y,parameters)\n",
    "    loss = loss/n #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543332496313484"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=-\\Sigma[(y_i-\\theta^{T}X_i)X_{ij}]$\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)=-\\Sigma[(y_i-p_i)X_{ij}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model):\n",
    "    if model == 'linear':\n",
    "        # 위에서 정의한 dot_product()함수를 통해 y_hat을 구하기 (벡터나 행렬의 내적은 합을 동반하므로 sigma는 따로 필요없음)\n",
    "        y_hat = dot_product(X,parameters)\n",
    "        gradient = -(y-y_hat)*X[j]\n",
    "    else:\n",
    "        # 위에서 만든 logistic()함수를 통해 시그모이드 함수 구하고 이 함수의 gradient까지 도출하기\n",
    "        p = logistic(X,parameters)\n",
    "        gradient = -(y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뒤에서 루프를 통해 여러 행(batch dataset)에 대해 다룰 것이므로 위 함수에서는 하나의 행에 대해서만 gradient를 출력하도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11993229782510513"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07713917320172926"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X,y,parameters,j,model)\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.18862239882450849, 128.9466985990136, 128.63939887550058]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'linear')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.256254251138344, 13.472617088395454, 38.47699599735951]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       " array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
       " array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69]),\n",
       " array([70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n",
       " array([80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([100, 101, 102, 103, 104, 105, 106, 107, 108, 109]),\n",
       " array([110, 111, 112, 113, 114, 115, 116, 117, 118, 119]),\n",
       " array([120, 121, 122, 123, 124, 125, 126, 127, 128, 129]),\n",
       " array([130, 131, 132, 133, 134, 135, 136, 137, 138, 139]),\n",
       " array([140, 141, 142, 143, 144, 145, 146, 147, 148, 149])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_idx(X_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요 \n",
    "\n",
    "\n",
    "> **설명 : N은 전체 데이터셋의 행의 개수이고, batch_size는 최적화 과정(기울기 갱신) 한번당 쓰일 데이터셋의 행의 개수이다. idx는 0부터 N-1까지의 인덱스를 나열한 배열로, 반복문을 통해 i번째 배치와 i+1번째 배치 전까지의 인덱스를 idx_list에 넣어 출력하는 데에 사용된다. 배치의 크기가 커질수록 데이터셋의 개수는 줄어들며, 배치의 크기가 줄어들면 데이터셋의 개수는 커진다. 따라서 배치의 크기는 전체데이터셋의 크기를 고려하여 결정되어야 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)):\n",
    "        gradients[i] *= learning_rate/n\n",
    "    \n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28124491, 0.91156859, 0.07228485])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 현재 반복 횟수  \n",
    "- num_epoch: 총 반복 횟수  \n",
    "<br>\n",
    "\n",
    "BGD: 학습 한 번에 모든 데이터셋의 대해 기울기를 계산  \n",
    "SGD: 학습 한 번에 임의의 데이터 하나에 대해서만 기울기를 계산  \n",
    "MGD: 학습 한 번에 데이터셋 일부에 대해서만 기울기를 계산  \n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요  \n",
    "batch_size=1 -> SGD  \n",
    "batch_size=k -> MGD  \n",
    "batch_size=whole -> BGD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, len(X_batch))\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.9345260618607345  params: [0.53154578 0.21204716 0.57486158]  gradients: [0.03406776557222189, 0.0017403441616958368, 0.025054675305689274]\n",
      "epoch: 100  loss: 0.4693508325266181  params: [-0.80502409  0.75616567 -0.67338519]  gradients: [0.003889736208241533, -0.0067259919882686125, 0.007056337229902948]\n",
      "epoch: 200  loss: 0.39933845350156494  params: [-1.02043771  1.32276439 -1.24033255]  gradients: [0.0013131530254901725, -0.004743856659342771, 0.004652939549718379]\n",
      "epoch: 300  loss: 0.3655963224005288  params: [-1.12665847  1.73012919 -1.63795032]  gradients: [0.0009022701817583008, -0.003519458958022816, 0.003420376716395757]\n",
      "epoch: 400  loss: 0.34616042903202765  params: [-1.20777902  2.04077798 -1.93884221]  gradients: [0.0007360676260198314, -0.0027555174956714686, 0.002660052603896285]\n",
      "epoch: 500  loss: 0.3338234049526657  params: [-1.27537239  2.28876069 -2.17756751]  gradients: [0.0006222149734966432, -0.002239994442867347, 0.002150449283212491]\n",
      "epoch: 600  loss: 0.32546806305119963  params: [-1.33294684  2.4931341  -2.37331445]  gradients: [0.0005335461014988651, -0.0018699792602562678, 0.0017869520071764013]\n",
      "epoch: 700  loss: 0.3195420997131908  params: [-1.3825831   2.6654711  -2.53767502]  gradients: [0.0004624010268222149, -0.001591846163318942, 0.001515232711263075]\n",
      "epoch: 800  loss: 0.3151921539838081  params: [-1.42580128  2.81329705 -2.67814831]  gradients: [0.0004044571824851055, -0.0013753196427684752, 0.0013047468199529828]\n",
      "epoch: 900  loss: 0.31191328924246947  params: [-1.46375845  2.94177691 -2.7998567 ]  gradients: [0.0003566430327416835, -0.0012021041544033435, 0.0011371129783118179]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.49703086,  3.05354995, -2.90545255])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train, batch_size=X_train.shape[0])\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BGD방식은 한번에 모든 데이터셋에 대해 기울기를 계산하기에 속도는 가장 빠르지만 지역 최적화의 문제가 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.2766023839579943  params: [-0.87286904  1.15226048 -1.27613877]  gradients: [0.02499427591061571, 0.013601908856784356, 0.017585741128443494]\n",
      "epoch: 100  loss: 0.07736668581128785  params: [-1.93032598  4.17502036 -4.06769192]  gradients: [0.007538540675391516, 0.004102481045921648, 0.005304047425809625]\n",
      "epoch: 200  loss: 0.07736266551921321  params: [-1.9303681   4.1751431  -4.06780374]  gradients: [0.007538159521584201, 0.004102273621655146, 0.005303779249519601]\n",
      "epoch: 300  loss: 0.07736266518361704  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489767116, 0.0041022736043402576, 0.0053037792271333935]\n",
      "epoch: 400  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 500  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 600  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 700  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 800  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n",
      "epoch: 900  loss: 0.07736266518359017  params: [-1.9303681   4.17514311 -4.06780375]  gradients: [0.007538159489764571, 0.004102273604338872, 0.005303779227131603]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9303681 ,  4.17514311, -4.06780375])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, batch_size=1)\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SGD방식은 한번에 임의의 데이터 하나에 대해 기울기를 계산하기에 속도는 BGD방식에 비해 많이 느리지만 지역최적화에 빠질 위험성이 상대적으로 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0122366048889482  params: [0.01192712 0.0096865  0.63518556]  gradients: [0.033470753062868695, 0.027807125519592312, 0.06432962453321321]\n",
      "epoch: 100  loss: 0.2359406278153507  params: [-1.49471986  3.04326427 -2.90108201]  gradients: [0.0017907305545148855, -0.0009585788878633271, 0.003131109689536421]\n",
      "epoch: 200  loss: 0.2113847706696736  params: [-1.69579934  3.70355844 -3.51934594]  gradients: [0.0026975533964774174, -0.001048028911913541, 0.0012863950657978703]\n",
      "epoch: 300  loss: 0.20352024082534567  params: [-1.78108305  3.97993031 -3.7755878 ]  gradients: [0.0030970327680041147, -0.0010448678417763818, 0.0006710729169669334]\n",
      "epoch: 400  loss: 0.20015252242918447  params: [-1.82235418  4.11314608 -3.89859494]  gradients: [0.003289466422008359, -0.0010378555925006402, 0.00040071826833665064]\n",
      "epoch: 500  loss: 0.19853498287179383  params: [-1.84341133  4.18099885 -3.96112527]  gradients: [0.0033870752239882193, -0.0010332045370942083, 0.0002689560402161268]\n",
      "epoch: 600  loss: 0.19771497116684084  params: [-1.85442433  4.21645678 -3.9937694 ]  gradients: [0.0034379296852515115, -0.0010305200220095745, 0.00020162077443418354]\n",
      "epoch: 700  loss: 0.19728781465613732  params: [-1.86025609  4.235225   -4.01103931]  gradients: [0.003464798986591591, -0.0010290334236814321, 0.00016639150750383342]\n",
      "epoch: 800  loss: 0.19706213904332254  params: [-1.86336411  4.24522526 -4.0202387 ]  gradients: [0.0034791011919222087, -0.0010282234731604095, 0.00014773526493950663]\n",
      "epoch: 900  loss: 0.19694201852748255  params: [-1.86502614  4.25057231 -4.02515683]  gradients: [0.0034867441868131534, -0.0010277854151412892, 0.0001377924891659333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.86591008,  4.25341594, -4.02777214])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_mgd = gradient_descent(X_train, y_train, batch_size=15)\n",
    "new_param_mgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MGD방식은 SGD와 BGD방식을 혼합한 것으로 속도는 SGD보다 빠르고, 지역 최적화에 빠질 위험성은 BGD보다 낮다는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_param_bgd를 활용한 에측\n",
    "y_predict_bgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict_bgd.append(1)\n",
    "    else :\n",
    "        y_predict_bgd.append(0)\n",
    "        \n",
    "# new_param_sgd를 활용한 예측\n",
    "y_predict_sgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_sgd)\n",
    "    if p > 0.5 :\n",
    "        y_predict_sgd.append(1)\n",
    "    else :\n",
    "        y_predict_sgd.append(0)\n",
    "\n",
    "# new_param_mgd를 활용한 예측\n",
    "y_predict_mgd = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_mgd)\n",
    "    if p > 0.5 :\n",
    "        y_predict_mgd.append(1)\n",
    "    else :\n",
    "        y_predict_mgd.append(0)\n",
    "        \n",
    "# 파라미터를 랜덤으로하여 예측\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGD방식으로 예측한 오차행렬 \n",
      " [[38  2]\n",
      " [ 4  6]] \n",
      " accuracy: 0.88 \n",
      "\n",
      "SGD방식으로 예측한 오차행렬 \n",
      " [[38  2]\n",
      " [ 1  9]] \n",
      " accuracy: 0.94 \n",
      "\n",
      "MGD방식으로 예측한 오차행렬 \n",
      " [[38  2]\n",
      " [ 1  9]] \n",
      " accuracy: 0.94 \n",
      "\n",
      "랜덤방식으로 예측한 오차행렬 \n",
      " [[13 27]\n",
      " [ 1  9]] \n",
      " accuracy: 0.44 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=['BGD','SGD','MGD','랜덤']\n",
    "for i, y_predict in enumerate([y_predict_bgd, y_predict_sgd, y_predict_mgd, y_predict_random]):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "    accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "    print(f'{a[i]}방식으로 예측한 오차행렬','\\n',confusion_matrix(y_test, y_predict),'\\n',\"accuracy:\",accuracy,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45845902, 2.92911047])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0800821785061923  params: [0.81333132 0.52462177]  gradients: [-0.09002222359652245, -0.0678587636885969]\n",
      "epoch: 100  loss: 0.35566956145592843  params: [0.50285637 2.85833489]  gradients: [-0.013339524981781403, -0.01267084049019623]\n",
      "epoch: 200  loss: 0.35172498078473535  params: [0.46559215 2.93054234]  gradients: [-0.014101680185225136, -0.012289119296642002]\n",
      "epoch: 300  loss: 0.3515905333574132  params: [0.46422254 2.93319624]  gradients: [-0.01412969235396421, -0.012275089557768883]\n",
      "epoch: 400  loss: 0.35158560611755163  params: [0.46417221 2.93329378]  gradients: [-0.014130721910141145, -0.012274573910285377]\n",
      "epoch: 500  loss: 0.35158542504153567  params: [0.46417036 2.93329737]  gradients: [-0.014130759750335904, -0.012274554958234344]\n",
      "epoch: 600  loss: 0.35158541838631374  params: [0.46417029 2.9332975 ]  gradients: [-0.014130761141110268, -0.012274554261672748]\n",
      "epoch: 700  loss: 0.3515854181417083  params: [0.46417028 2.93329751]  gradients: [-0.014130761192226638, -0.012274554236071393]\n",
      "epoch: 800  loss: 0.351585418132718  params: [0.46417028 2.93329751]  gradients: [-0.014130761194105371, -0.01227455423513043]\n",
      "epoch: 900  loss: 0.35158541813238775  params: [0.46417028 2.93329751]  gradients: [-0.014130761194174413, -0.012274554235095856]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.46417028, 2.93329751])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, batch_size = 30, model='linear')\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규방정식과 경사하강법으로 구한 회귀선이 거의 일치하는 것을 확인할 수 있다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
